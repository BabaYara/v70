---
title: Theoretical Properties for Neural Networks with Weight Matrices of Low Displacement
  Rank
booktitle: Proceedings of the 34th International Conference on Machine Learning
year: '2017'
volume: '70'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v70/zhao17b/zhao17b.pdf
url: http://proceedings.mlr.press/v70/zhao17b.html
abstract: Recently low displacement rank (LDR) matrices, or so-called structured matrices,
  have been proposed to compress large-scale neural networks. Empirical results have
  shown that neural networks with weight matrices of LDR matrices, referred as LDR
  neural networks, can achieve significant reduction in space and computational complexity
  while retaining high accuracy. This paper gives theoretical study on LDR neural
  networks. First, we prove the universal approximation property of LDR neural networks
  with a mild condition on the displacement operators. We then show that the error
  bounds of LDR neural networks are as efficient as general neural networks with both
  single-layer and multiple-layer structure. Finally, we propose back-propagation
  based training algorithm for general LDR neural networks.
layout: inproceedings
id: zhao17b
tex_title: Theoretical Properties for Neural Networks with Weight Matrices of Low
  Displacement Rank
firstpage: 4082
lastpage: 4090
page: 4082-4090
order: 4082
cycles: false
editor:
- given: Doina
  family: Precup
- given: Yee Whye
  family: Teh
author:
- given: Liang
  family: Zhao
- given: Siyu
  family: Liao
- given: Yanzhi
  family: Wang
- given: Zhe
  family: Li
- given: Jian
  family: Tang
- given: Bo
  family: Yuan
date: 2017-07-17
container-title: Proceedings of the 34th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 7
  - 17
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---

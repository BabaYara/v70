---
title: Approximate Newton Methods and Their Local Convergence
booktitle: Proceedings of the 34th International Conference on Machine Learning
year: '2017'
volume: '70'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v70/ye17a/ye17a.pdf
url: http://proceedings.mlr.press/v70/ye17a.html
abstract: Many machine learning models are reformulated as optimization problems.
  Thus, it is important to solve a large-scale optimization problem in big data applications.
  Recently, subsampled Newton methods have emerged to attract much attention for optimization
  due to their efficiency at each iteration, rectified a weakness in the ordinary
  Newton method of suffering a high cost in each iteration while commanding a high
  convergence rate. Other efficient stochastic second order methods are also proposed.
  However, the convergence properties of these methods are still not well understood.
  There are also several important gaps between the current convergence theory and
  performance in real applications. In this paper, we aim to fill these gaps. We propose
  a unifying framework to analyze local convergence properties of second order methods.
  Based on this framework, our theoretical analysis matches the performance in real
  applications.
layout: inproceedings
id: ye17a
tex_title: Approximate {N}ewton Methods and Their Local Convergence
firstpage: 3931
lastpage: 3939
page: 3931-3939
order: 3931
cycles: false
editor:
- given: Doina
  family: Precup
- given: Yee Whye
  family: Teh
author:
- given: Haishan
  family: Ye
- given: Luo
  family: Luo
- given: Zhihua
  family: Zhang
date: 2017-07-17
container-title: Proceedings of the 34th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 7
  - 17
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v70/ye17a/ye17a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---

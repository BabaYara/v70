---
title: Efficient softmax approximation for GPUs
booktitle: Proceedings of the 34th International Conference on Machine Learning
year: '2017'
volume: '70'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v70/grave17a/grave17a.pdf
url: http://proceedings.mlr.press/v70/grave17a.html
abstract: We propose an approximate strategy to efficiently train neural network based
  language models over very large vocabularies. Our approach, called adaptive softmax,
  circumvents the linear dependency on the vocabulary size by exploiting the unbalanced
  word distribution to form clusters that explicitly minimize the expectation of computation
  time. Our approach further reduces the computational cost by exploiting the specificities
  of modern architectures and matrix-matrix vector operations, making it particularly
  suited for graphical processing units. Our experiments carried out on standard benchmarks,
  such as EuroParl and One Billion Word, show that our approach brings a large gain
  in efficiency over standard approximations while achieving an accuracy close to
  that of the full softmax. The code of our method is available at https://github.com/facebookresearch/adaptive-softmax.
layout: inproceedings
id: grave17a
tex_title: Efficient softmax approximation for {GPU}s
bibtex_author: "{\\'E}douard Grave and Armand Joulin and Moustapha Ciss{\\'e} and\
  \ David Grangier and Herv{\\'e} J{\\'e}gou"
firstpage: 1302
lastpage: 1310
page: 1302-1310
order: 1302
cycles: false
editor:
- given: Doina
  family: Precup
- given: Yee Whye
  family: Teh
author:
- given: ''
  family: Grave
- given: Armand
  family: Joulin
- given: Moustapha
  family: Cissé
- given: David
  family: Grangier
- given: Hervé
  family: Jégou
date: 2017-07-17
container-title: Proceedings of the 34th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 7
  - 17
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---

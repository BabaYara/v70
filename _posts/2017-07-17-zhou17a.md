---
title: Stochastic Adaptive Quasi-Newton Methods for Minimizing Expected Values
booktitle: Proceedings of the 34th International Conference on Machine Learning
year: '2017'
volume: '70'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v70/zhou17a/zhou17a.pdf
url: http://proceedings.mlr.press/v70/zhou17a.html
abstract: We propose a novel class of stochastic, adaptive methods for minimizing
  self-concordant functions which can be expressed as an expected value. These methods
  generate an estimate of the true objective function by taking the empirical mean
  over a sample drawn at each step, making the problem tractable. The use of adaptive
  step sizes eliminates the need for the user to supply a step size. Methods in this
  class include extensions of gradient descent (GD) and BFGS. We show that, given
  a suitable amount of sampling, the stochastic adaptive GD attains linear convergence
  in expectation, and with further sampling, the stochastic adaptive BFGS attains
  R-superlinear convergence. We present experiments showing that these methods compare
  favorably to SGD.
layout: inproceedings
id: zhou17a
tex_title: Stochastic Adaptive Quasi-{N}ewton Methods for Minimizing Expected Values
bibtex_author: Chaoxu Zhou and Wenbo Gao and Donald Goldfarb
firstpage: 4150
lastpage: 4159
page: 4150-4159
order: 4150
cycles: false
editor:
- given: Doina
  family: Precup
- given: Yee Whye
  family: Teh
author:
- given: Chaoxu
  family: Zhou
- given: Wenbo
  family: Gao
- given: Donald
  family: Goldfarb
date: 2017-07-17
container-title: Proceedings of the 34th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 7
  - 17
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v70/zhou17a/zhou17a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---

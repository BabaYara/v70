---
title: Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs
booktitle: Proceedings of the 34th International Conference on Machine Learning
year: '2017'
volume: '70'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v70/brutzkus17a/brutzkus17a.pdf
url: http://proceedings.mlr.press/v70/brutzkus17a.html
abstract: Deep learning models are often successfully trained using gradient descent,
  despite the worst case hardness of the underlying non-convex optimization problem.
  The key question is then under what conditions can one prove that optimization will
  succeed. Here we provide a strong result of this kind. We consider a neural net
  with one hidden layer and a convolutional structure with no overlap and a ReLU activation
  function. For this architecture we show that learning is NP-complete in the general
  case, but that when the input distribution is Gaussian, gradient descent converges
  to the global optimum in polynomial time. To the best of our knowledge, this is
  the first global optimality guarantee of gradient descent on a convolutional neural
  network with ReLU activations.
layout: inproceedings
id: brutzkus17a
tex_title: Globally Optimal Gradient Descent for a {C}onv{N}et with {G}aussian Inputs
bibtex_author: Alon Brutzkus and Amir Globerson
firstpage: 605
lastpage: 614
page: 605-614
order: 605
cycles: false
editor:
- given: Doina
  family: Precup
- given: Yee Whye
  family: Teh
author:
- given: Alon
  family: Brutzkus
- given: Amir
  family: Globerson
date: 2017-07-17
container-title: Proceedings of the 34th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 7
  - 17
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v70/brutzkus17a/brutzkus17a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---

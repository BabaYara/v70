---
title: Learning to Learn without Gradient Descent by Gradient Descent
booktitle: Proceedings of the 34th International Conference on Machine Learning
year: '2017'
volume: '70'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v70/chen17e/chen17e.pdf
url: http://proceedings.mlr.press/v70/chen17e.html
abstract: We learn recurrent neural network optimizers trained on simple synthetic
  functions by gradient descent. We show that these learned optimizers exhibit a remarkable
  degree of transfer in that they can be used to efficiently optimize a broad range
  of derivative-free black-box functions, including Gaussian process bandits, simple
  control objectives, global optimization benchmarks and hyper-parameter tuning tasks.
  Up to the training horizon, the learned optimizers learn to trade-off exploration
  and exploitation, and compare favourably with heavily engineered Bayesian optimization
  packages for hyper-parameter tuning.
layout: inproceedings
id: chen17e
tex_title: Learning to Learn without Gradient Descent by Gradient Descent
firstpage: 748
lastpage: 756
page: 748-756
order: 748
cycles: false
editor:
- given: Doina
  family: Precup
- given: Yee Whye
  family: Teh
author:
- given: Yutian
  family: Chen
- given: Matthew W.
  family: Hoffman
- given: Sergio GÃ³mez
  family: Colmenarejo
- given: Misha
  family: Denil
- given: Timothy P.
  family: Lillicrap
- given: Matt
  family: Botvinick
- given: Nando
  family: Freitas
date: 2017-07-17
container-title: Proceedings of the 34th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 7
  - 17
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---

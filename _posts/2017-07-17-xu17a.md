---
title: 'Stochastic Convex Optimization: Faster Local Growth Implies Faster Global
  Convergence'
booktitle: Proceedings of the 34th International Conference on Machine Learning
year: '2017'
volume: '70'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v70/xu17a/xu17a.pdf
url: http://proceedings.mlr.press/v70/xu17a.html
abstract: 'In this paper, a new theory is developed for first-order stochastic convex
  optimization, showing that the global convergence rate is sufficiently quantified
  by a local growth rate of the objective function in a neighborhood of the optimal
  solutions. In particular, if the objective function $F(\mathbfw)$ in the $ε$-sublevel
  set grows as fast as $\|\mathbfw - \mathbfw_*\|_2^1/θ$, where $\mathbfw_*$ represents
  the closest optimal solution to $\mathbfw$ and $θ∈(0,1]$ quantifies the local growth
  rate, the iteration complexity of first-order stochastic optimization for achieving
  an $ε$-optimal solution can be $\widetilde O(1/ε^2(1-θ))$, which is \emphoptimal
  at most up to a logarithmic factor. This result is fundamentally better in contrast
  with the previous works that either assume a global growth condition in the entire
  domain or achieve a local faster convergence under the local faster growth condition.
  To achieve the faster global convergence, we develop two different \bf accelerated
  stochastic subgradient methods by iteratively solving the original problem approximately
  in a local region around a historical solution with the size of the local region
  gradually decreasing as the solution approaches the optimal set. Besides the theoretical
  improvements, this work also include new contributions towards making the proposed
  algorithms practical: (i) we present practical variants of accelerated stochastic
  subgradient methods that can run without the knowledge of multiplicative growth
  constant and even the growth rate $θ$; (ii) we consider a broad family of problems
  in machine learning to demonstrate that the proposed algorithms enjoy faster convergence
  than traditional stochastic subgradient method. For example, when applied to the
  $\ell_1$ regularized empirical polyhedral loss minimization (e.g., hinge loss, absolute
  loss), the proposed stochastic methods have a logarithmic iteration complexity.'
layout: inproceedings
id: xu17a
tex_title: 'Stochastic Convex Optimization: Faster Local Growth Implies Faster Global
  Convergence'
firstpage: 3821
lastpage: 3830
page: 3821-3830
order: 3821
cycles: false
editor:
- given: Doina
  family: Precup
- given: Yee Whye
  family: Teh
author:
- given: Yi
  family: Xu
- given: Qihang
  family: Lin
- given: Tianbao
  family: Yang
date: 2017-07-17
container-title: Proceedings of the 34th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 7
  - 17
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v70/xu17a/xu17a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---

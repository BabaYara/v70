---
title: 'Stochastic Convex Optimization: Faster Local Growth Implies Faster Global
  Convergence'
booktitle: Proceedings of the 34th International Conference on Machine Learning
year: '2017'
volume: '70'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v70/xu17a/xu17a.pdf
url: http://proceedings.mlr.press/v70/xu17a.html
abstract: 'In this paper, a new theory is developed for first-order stochastic convex
  optimization, showing that the global convergence rate is sufficiently quantified
  by a local growth rate of the objective function in a neighborhood of the optimal
  solutions. In particular, if the objective function $F(\mathbf{w})$ in the $\epsilon$-sublevel
  set grows as fast as $\|\mathbf{w} - \mathbf{w}_*\|_2^{1/\theta}$, where $\mathbf{w}_*$
  represents the closest optimal solution to $\mathbf{w}$ and $\theta\in(0,1]$ quantifies
  the local growth rate, the iteration complexity of first-order stochastic optimization
  for achieving an $\epsilon$-optimal solution can be $\widetilde O(1/\epsilon^{2(1-\theta)})$,
  which is <em>optimal at most</em> up to a logarithmic factor. This result is fundamentally
  better in contrast with the previous works that either assume a global growth condition
  in the entire domain or achieve a local faster convergence under the local faster
  growth condition. To achieve the faster global convergence, we develop two different
  <em>accelerated stochastic subgradient</em> methods by iteratively solving the original
  problem approximately in a local region around a historical solution with the size
  of the local region gradually decreasing as the solution approaches the optimal
  set. Besides the theoretical improvements, this work also include new contributions
  towards making the proposed algorithms practical: (i) we present practical variants
  of accelerated stochastic subgradient methods that can run without the knowledge
  of multiplicative growth constant and even the growth rate $\theta$; (ii) we consider
  a broad family of problems in machine learning to demonstrate that the proposed
  algorithms enjoy faster convergence than traditional stochastic subgradient method.
  For example, when applied to the $\ell_1$ regularized empirical polyhedral loss
  minimization (e.g., hinge loss, absolute loss), the proposed stochastic methods
  have a logarithmic iteration complexity.'
layout: inproceedings
id: xu17a
tex_title: 'Stochastic Convex Optimization: Faster Local Growth Implies Faster Global
  Convergence'
firstpage: 3821
lastpage: 3830
page: 3821-3830
order: 3821
cycles: false
editor:
- given: Doina
  family: Precup
- given: Yee Whye
  family: Teh
author:
- given: Yi
  family: Xu
- given: Qihang
  family: Lin
- given: Tianbao
  family: Yang
date: 2017-07-17
container-title: Proceedings of the 34th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 7
  - 17
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v70/xu17a/xu17a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---

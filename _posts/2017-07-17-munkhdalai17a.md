---
title: Meta Networks
booktitle: Proceedings of the 34th International Conference on Machine Learning
year: '2017'
volume: '70'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v70/munkhdalai17a/munkhdalai17a.pdf
url: http://proceedings.mlr.press/v70/munkhdalai17a.html
abstract: Neural networks have been successfully applied in applications with a large
  amount of labeled data. However, the task of rapid generalization on new concepts
  with small training data while preserving performances on previously learned ones
  still presents a significant challenge to neural network models. In this work, we
  introduce a novel meta learning method, Meta Networks (MetaNet), that learns a meta-level
  knowledge across tasks and shifts its inductive biases via fast parameterization
  for rapid generalization. When evaluated on Omniglot and Mini-ImageNet benchmarks,
  our MetaNet models achieve a near human-level performance and outperform the baseline
  approaches by up to 6% accuracy. We demonstrate several appealing properties of
  MetaNet relating to generalization and continual learning.
layout: inproceedings
id: munkhdalai17a
tex_title: Meta Networks
firstpage: 2554
lastpage: 2563
page: 2554-2563
order: 2554
cycles: false
editor:
- given: Doina
  family: Precup
- given: Yee Whye
  family: Teh
author:
- given: Tsendsuren
  family: Munkhdalai
- given: Hong
  family: Yu
date: 2017-07-17
container-title: Proceedings of the 34th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 7
  - 17
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v70/munkhdalai17a/munkhdalai17a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---

---
title: Variants of RMSProp and Adagrad with Logarithmic Regret Bounds
booktitle: Proceedings of the 34th International Conference on Machine Learning
year: '2017'
volume: '70'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v70/mukkamala17a/mukkamala17a.pdf
url: http://proceedings.mlr.press/v70/mukkamala17a.html
abstract: Adaptive gradient methods have become recently very popular, in particular
  as they have been shown to be useful in the training of deep neural networks. In
  this paper we have analyzed RMSProp, originally proposed for the training of deep
  neural networks, in the context of online convex optimization and show $\sqrt{T}$-type
  regret bounds. Moreover, we propose two variants SC-Adagrad and SC-RMSProp for which
  we show logarithmic regret bounds for strongly convex functions. Finally, we demonstrate
  in the experiments that these new variants outperform other adaptive gradient techniques
  or stochastic gradient descent in the optimization of strongly convex functions
  as well as in training of deep neural networks.
layout: inproceedings
id: mukkamala17a
tex_title: Variants of {RMSP}rop and {A}dagrad with Logarithmic Regret Bounds
firstpage: 2545
lastpage: 2553
page: 2545-2553
order: 2545
cycles: false
editor:
- given: Doina
  family: Precup
- given: Yee Whye
  family: Teh
author:
- given: Mahesh Chandra
  family: Mukkamala
- given: Matthias
  family: Hein
date: 2017-07-17
container-title: Proceedings of the 34th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 7
  - 17
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v70/mukkamala17a/mukkamala17a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---

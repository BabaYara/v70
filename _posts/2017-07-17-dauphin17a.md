---
title: Language Modeling with Gated Convolutional Networks
booktitle: Proceedings of the 34th International Conference on Machine Learning
year: '2017'
volume: '70'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v70/dauphin17a/dauphin17a.pdf
url: http://proceedings.mlr.press/v70/dauphin17.html
abstract: The pre-dominant approach to language modeling to date is based on recurrent
  neural networks. Their success on this task is often linked to their ability to
  capture unbounded context. In this paper we develop a finite context approach through
  stacked convolutions, which can be more efficient since they allow parallelization
  over sequential tokens. We propose a novel simplified gating mechanism that outperforms
  Oord et al. (2016) and investigate the impact of key architectural decisions. The
  proposed approach achieves state-of-the-art on the WikiText-103 benchmark, even
  though it features long-term dependencies, as well as competitive results on the
  Google Billion Words benchmark. Our model reduces the latency to score a sentence
  by an order of magnitude compared to a recurrent baseline. To our knowledge, this
  is the first time a non-recurrent approach is competitive with strong recurrent
  models on these large scale language tasks.
layout: inproceedings
id: dauphin17a
tex_title: Language Modeling with Gated Convolutional Networks
firstpage: 933
lastpage: 941
page: 933-941
order: 933
cycles: false
editor:
- given: Doina
  family: Precup
- given: Yee Whye
  family: Teh
author:
- given: Yann N.
  family: Dauphin
- given: Angela
  family: Fan
- given: Michael
  family: Auli
- given: David
  family: Grangier
date: 2017-07-17
container-title: Proceedings of the 34th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 7
  - 17
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---

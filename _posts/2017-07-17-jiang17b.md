---
title: Uniform Convergence Rates for Kernel Density Estimation
booktitle: Proceedings of the 34th International Conference on Machine Learning
year: '2017'
volume: '70'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v70/jiang17b/jiang17b.pdf
url: http://proceedings.mlr.press/v70/jiang17b.html
abstract: Kernel density estimation (KDE) is a popular nonparametric density estimation
  method. We (1) derive finite-sample high-probability density estimation bounds for
  multivariate KDE under mild density assumptions which hold uniformly in $x \in \mathbb{R}^d$
  and bandwidth matrices. We apply these results to (2) mode, (3) density level set,
  and (4) class probability estimation and attain optimal rates up to logarithmic
  factors. We then (5) provide an extension of our results under the manifold hypothesis.
  Finally, we (6) give uniform convergence results for local intrinsic dimension estimation.
layout: inproceedings
id: jiang17b
tex_title: Uniform Convergence Rates for Kernel Density Estimation
bibtex_author: Heinrich Jiang
firstpage: 1694
lastpage: 1703
page: 1694-1703
order: 1694
cycles: false
editor:
- given: Doina
  family: Precup
- given: Yee Whye
  family: Teh
author:
- given: Heinrich
  family: Jiang
date: 2017-07-17
container-title: Proceedings of the 34th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 7
  - 17
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v70/jiang17b/jiang17b-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---

---
title: An Alternative Softmax Operator for Reinforcement Learning
booktitle: Proceedings of the 34th International Conference on Machine Learning
year: '2017'
volume: '70'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v70/asadi17a/asadi17a.pdf
url: http://proceedings.mlr.press/v70/asadi17.html
abstract: A softmax operator applied to a set of values acts somewhat like the maximization
  function and somewhat like an average. In sequential decision making, softmax is
  often used in settings where it is necessary to maximize utility but also to hedge
  against problems that arise from putting all of oneâ€™s weight behind a single maximum
  utility decision. The Boltzmann softmax operator is the most commonly used softmax
  operator in this setting, but we show that this operator is prone to misbehavior.
  In this work, we study a differentiable softmax operator that, among other properties,
  is a non-expansion ensuring a convergent behavior in learning and planning. We introduce
  a variant of SARSA algorithm that, by utilizing the new operator, computes a Boltzmann
  policy with a state-dependent temperature parameter. We show that the algorithm
  is convergent and that it performs favorably in practice.
layout: inproceedings
id: asadi17a
tex_title: An Alternative Softmax Operator for Reinforcement Learning
firstpage: 243
lastpage: 252
page: 243-252
order: 243
cycles: false
editor:
- given: Doina
  family: Precup
- given: Yee Whye
  family: Teh
author:
- given: Kavosh
  family: Asadi
- given: Michael L.
  family: Littman
date: 2017-07-17
container-title: Proceedings of the 34th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 7
  - 17
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---

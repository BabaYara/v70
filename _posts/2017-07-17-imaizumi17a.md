---
title: Tensor Decomposition with Smoothness
booktitle: Proceedings of the 34th International Conference on Machine Learning
year: '2017'
volume: '70'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v70/imaizumi17a/imaizumi17a.pdf
url: http://proceedings.mlr.press/v70/imaizumi17a.html
abstract: Real data tensors are usually high dimensional but their intrinsic information
  is preserved in low-dimensional space, which motivates to use tensor decompositions
  such as Tucker decomposition. Often, real data tensors are not only low dimensional,
  but also smooth, meaning that the adjacent elements are similar or continuously
  changing, which typically appear as spatial or temporal data. To incorporate the
  smoothness property, we propose the smoothed Tucker decomposition (STD). STD leverages
  the smoothness by the sum of a few basis functions, which reduces the number of
  parameters. The objective function is formulated as a convex problem and, to solve
  that, an algorithm based on the alternating direction method of multipliers is derived.
  We theoretically show that, under the smoothness assumption, STD achieves a better
  error bound. The theoretical result and performances of STD are numerically verified.
layout: inproceedings
id: imaizumi17a
tex_title: Tensor Decomposition with Smoothness
bibtex_author: Masaaki Imaizumi and Kohei Hayashi
firstpage: 1597
lastpage: 1606
page: 1597-1606
order: 1597
cycles: false
editor:
- given: Doina
  family: Precup
- given: Yee Whye
  family: Teh
author:
- given: Masaaki
  family: Imaizumi
- given: Kohei
  family: Hayashi
date: 2017-07-17
container-title: Proceedings of the 34th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 7
  - 17
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v70/imaizumi17a/imaizumi17a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---

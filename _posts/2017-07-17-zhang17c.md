---
title: Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample
  Reduction
booktitle: Proceedings of the 34th International Conference on Machine Learning
year: '2017'
volume: '70'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v70/zhang17c/zhang17c.pdf
url: http://proceedings.mlr.press/v70/zhang17c.html
abstract: Sparse support vector machine (SVM) is a popular classification technique
  that can simultaneously learn a small set of the most interpretable features and
  identify the support vectors. It has achieved great successes in many real-world
  applications. However, for large-scale problems involving a huge number of samples
  and extremely high-dimensional features, solving sparse SVMs remains challenging.
  By noting that sparse SVMs induce sparsities in both feature and sample spaces,
  we propose a novel approach, which is based on accurate estimations of the primal
  and dual optima of sparse SVMs, to simultaneously identify the features and samples
  that are guaranteed to be irrelevant to the outputs. Thus, we can remove the identified
  inactive samples and features from the training phase, leading to substantial savings
  in both the memory usage and computational cost without sacrificing accuracy. To
  the best of our knowledge, the proposed method is the <em>first</em> <em>static</em>
  feature and sample reduction method for sparse SVMs. Experiments on both synthetic
  and real datasets (e.g., the kddb dataset with about 20 million samples and 30 million
  features) demonstrate that our approach significantly outperforms state-of-the-art
  methods and the speedup gained by our approach can be orders of magnitude.
layout: inproceedings
id: zhang17c
tex_title: Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample
  Reduction
bibtex_author: Weizhong Zhang and Bin Hong and Wei Liu and Jieping Ye and Deng Cai
  and Xiaofei He and Jie Wang
firstpage: 4016
lastpage: 4025
page: 4016-4025
order: 4016
cycles: false
editor:
- given: Doina
  family: Precup
- given: Yee Whye
  family: Teh
author:
- given: Weizhong
  family: Zhang
- given: Bin
  family: Hong
- given: Wei
  family: Liu
- given: Jieping
  family: Ye
- given: Deng
  family: Cai
- given: Xiaofei
  family: He
- given: Jie
  family: Wang
date: 2017-07-17
container-title: Proceedings of the 34th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 7
  - 17
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v70/zhang17c/zhang17c-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---

---
title: On the Expressive Power of Deep Neural Networks
booktitle: Proceedings of the 34th International Conference on Machine Learning
year: '2017'
volume: '70'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v70/raghu17a/raghu17a.pdf
url: http://proceedings.mlr.press/v70/raghu17.html
abstract: 'We propose a new approach to the problem of neural network expressivity,
  which seeks to characterize how structural properties of a neural network family
  affect the functions it is able to compute. Our approach is based on an interrelated
  set of measures of expressivity, unified by the novel notion of trajectory length,
  which measures how the output of a network changes as the input sweeps along a one-dimensional
  path. Our findings show that: (1) The complexity of the computed function grows
  exponentially with depth (2) All weights are not equal: trained networks are more
  sensitive to their lower (initial) layer weights (3) Trajectory regularization is
  a simpler alternative to batch normalization, with the same performance.'
layout: inproceedings
id: raghu17a
tex_title: On the Expressive Power of Deep Neural Networks
firstpage: 2847
lastpage: 2854
page: 2847-2854
order: 2847
cycles: false
editor:
- given: Doina
  family: Precup
- given: Yee Whye
  family: Teh
author:
- given: Maithra
  family: Raghu
- given: Ben
  family: Poole
- given: Jon
  family: Kleinberg
- given: Surya
  family: Ganguli
- given: Jascha
  family: Sohl-Dickstein
date: 2017-07-17
container-title: Proceedings of the 34th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 7
  - 17
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v70/raghu17a/raghu17a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---

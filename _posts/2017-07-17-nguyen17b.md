---
title: 'SARAH: A Novel Method for Machine Learning Problems Using Stochastic Recursive
  Gradient'
booktitle: Proceedings of the 34th International Conference on Machine Learning
year: '2017'
volume: '70'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v70/nguyen17b/nguyen17b.pdf
url: http://proceedings.mlr.press/v70/nguyen17b.html
abstract: In this paper, we propose a StochAstic Recursive grAdient algoritHm (SARAH),
  as well as its practical variant SARAH+, as a novel approach to the finite-sum minimization
  problems. Different from the vanilla SGD and other modern stochastic methods such
  as SVRG, S2GD, SAG and SAGA, SARAH admits a simple recursive framework for updating
  stochastic gradient estimates; when comparing to SAG/SAGA, SARAH does not require
  a storage of past gradients. The linear convergence rate of SARAH is proven under
  strong convexity assumption. We also prove a linear convergence rate (in the strongly
  convex case) for an inner loop of SARAH, the property that SVRG does not possess.
  Numerical experiments demonstrate the efficiency of our algorithm.
layout: inproceedings
id: nguyen17b
tex_title: '{SARAH}: A Novel Method for Machine Learning Problems Using Stochastic
  Recursive Gradient'
firstpage: 2613
lastpage: 2621
page: 2613-2621
order: 2613
cycles: false
editor:
- given: Doina
  family: Precup
- given: Yee Whye
  family: Teh
author:
- given: Lam M.
  family: Nguyen
- given: Jie
  family: Liu
- given: Katya
  family: Scheinberg
- given: Martin
  family: Takáč
date: 2017-07-17
container-title: Proceedings of the 34th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 7
  - 17
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v70/nguyen17b/nguyen17b-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---

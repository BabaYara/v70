---
title: Variational Dropout Sparsifies Deep Neural Networks
booktitle: Proceedings of the 34th International Conference on Machine Learning
year: '2017'
volume: '70'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v70/molchanov17a/molchanov17a.pdf
url: http://proceedings.mlr.press/v70/molchanov17a.html
abstract: We explore a recently proposed Variational Dropout technique that provided
  an elegant Bayesian interpretation to Gaussian Dropout. We extend Variational Dropout
  to the case when dropout rates are unbounded, propose a way to reduce the variance
  of the gradient estimator and report first experimental results with individual
  dropout rates per weight. Interestingly, it leads to extremely sparse solutions
  both in fully-connected and convolutional layers. This effect is similar to automatic
  relevance determination effect in empirical Bayes but has a number of advantages.
  We reduce the number of parameters up to 280 times on LeNet architectures and up
  to 68 times on VGG-like networks with a negligible decrease of accuracy.
layout: inproceedings
id: molchanov17a
tex_title: Variational Dropout Sparsifies Deep Neural Networks
bibtex_author: Dmitry Molchanov and Arsenii Ashukha and Dmitry Vetrov
firstpage: 2498
lastpage: 2507
page: 2498-2507
order: 2498
cycles: false
editor:
- given: Doina
  family: Precup
- given: Yee Whye
  family: Teh
author:
- given: Dmitry
  family: Molchanov
- given: Arsenii
  family: Ashukha
- given: Dmitry
  family: Vetrov
date: 2017-07-17
container-title: Proceedings of the 34th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 7
  - 17
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---

---
title: Neural Networks and Rational Functions
booktitle: Proceedings of the 34th International Conference on Machine Learning
year: '2017'
volume: '70'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v70/telgarsky17a/telgarsky17a.pdf
url: http://proceedings.mlr.press/v70/telgarsky17.html
abstract: Neural networks and rational functions efficiently approximate each other.
  In more detail, it is shown here that for any ReLU network, there exists a rational
  function of degree $O(polylog(1/ε))$ which is $ε$-close, and similarly for any rational
  function there exists a ReLU network of size $O(polylog(1/ε))$ which is $ε$-close.
  By contrast, polynomials need degree $Ω(poly(1/ε))$ to approximate even a single
  ReLU. When converting a ReLU network to a rational function as above, the hidden
  constants depend exponentially on the number of layers, which is shown to be tight;
  in other words, a compositional representation can be beneficial even for rational
  functions.
layout: inproceedings
id: telgarsky17a
tex_title: Neural Networks and Rational Functions
firstpage: 3387
lastpage: 3393
page: 3387-3393
order: 3387
cycles: false
editor:
- given: Doina
  family: Precup
- given: Yee Whye
  family: Teh
author:
- given: Matus
  family: Telgarsky
date: 2017-07-17
container-title: Proceedings of the 34th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 7
  - 17
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v70/telgarsky17a/telgarsky17a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---

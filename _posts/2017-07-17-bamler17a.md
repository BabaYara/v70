---
title: Dynamic Word Embeddings
booktitle: Proceedings of the 34th International Conference on Machine Learning
year: '2017'
volume: '70'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v70/bamler17a/bamler17a.pdf
url: http://proceedings.mlr.press/v70/bamler17.html
abstract: We present a probabilistic language model for time-stamped text data which
  tracks the semantic evolution of individual words over time. The model represents
  words and contexts by latent trajectories in an embedding space. At each moment
  in time, the embedding vectors are inferred from a probabilistic version of word2vec
  [Mikolov et al., 2013]. These embedding vectors are connected in time through a
  latent diffusion process. We describe two scalable variational inference algorithms–skip-gram
  smoothing and skip-gram filtering–that allow us to train the model jointly over
  all times; thus learning on all data while simultaneously allowing word and context
  vectors to drift. Experimental results on three different corpora demonstrate that
  our dynamic model infers word embedding trajectories that are more interpretable
  and lead to higher predictive likelihoods than competing methods that are based
  on static models trained separately on time slices.
layout: inproceedings
id: bamler17a
tex_title: Dynamic Word Embeddings
firstpage: 380
lastpage: 389
page: 380-389
order: 380
cycles: false
editor:
- given: Doina
  family: Precup
- given: Yee Whye
  family: Teh
author:
- given: Robert
  family: Bamler
- given: Stephan
  family: Mandt
date: 2017-07-17
container-title: Proceedings of the 34th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 7
  - 17
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v70/bamler17a/bamler17a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---

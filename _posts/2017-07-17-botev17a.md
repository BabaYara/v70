---
title: Practical Gauss-Newton Optimisation for Deep Learning
booktitle: Proceedings of the 34th International Conference on Machine Learning
year: '2017'
volume: '70'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v70/botev17a/botev17a.pdf
url: http://proceedings.mlr.press/v70/botev17a.html
abstract: We present an efficient block-diagonal approximation to the Gauss-Newton
  matrix for feedforward neural networks. Our resulting algorithm is competitive against
  state-of-the-art first-order optimisation methods, with sometimes significant improvement
  in optimisation performance. Unlike first-order methods, for which hyperparameter
  tuning of the optimisation parameters is often a laborious process, our approach
  can provide good performance even when used with default settings. A side result
  of our work is that for piecewise linear transfer functions, the network objective
  function can have no differentiable local maxima, which may partially explain why
  such transfer functions facilitate effective optimisation.
layout: inproceedings
id: botev17a
tex_title: Practical {G}auss-{N}ewton Optimisation for Deep Learning
bibtex_author: Aleksandar Botev and Hippolyt Ritter and David Barber
firstpage: 557
lastpage: 565
page: 557-565
order: 557
cycles: false
editor:
- given: Doina
  family: Precup
- given: Yee Whye
  family: Teh
author:
- given: Aleksandar
  family: Botev
- given: Hippolyt
  family: Ritter
- given: David
  family: Barber
date: 2017-07-17
container-title: Proceedings of the 34th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 7
  - 17
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v70/botev17a/botev17a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---

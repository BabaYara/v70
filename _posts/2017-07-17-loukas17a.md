---
title: How Close Are the Eigenvectors of the Sample and Actual Covariance Matrices?
booktitle: Proceedings of the 34th International Conference on Machine Learning
year: '2017'
volume: '70'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v70/loukas17a/loukas17a.pdf
url: http://proceedings.mlr.press/v70/loukas17a.html
abstract: How many samples are sufficient to guarantee that the eigenvectors of the
  sample covariance matrix are close to those of the actual covariance matrix? For
  a wide family of distributions, including distributions with finite second moment
  and sub-gaussian distributions supported in a centered Euclidean ball, we prove
  that the inner product between eigenvectors of the sample and actual covariance
  matrices decreases proportionally to the respective eigenvalue distance and the
  number of samples. Our findings imply <em>non-asymptotic</em> concentration bounds
  for eigenvectors and eigenvalues and carry strong consequences for the non-asymptotic
  analysis of PCA and its applications. For instance, they provide conditions for
  separating components estimated from $O(1)$ samples and show that even few samples
  can be sufficient to perform dimensionality reduction, especially for low-rank covariances.
layout: inproceedings
id: loukas17a
tex_title: How Close Are the Eigenvectors of the Sample and Actual Covariance Matrices?
bibtex_author: Andreas Loukas
firstpage: 2228
lastpage: 2237
page: 2228-2237
order: 2228
cycles: false
editor:
- given: Doina
  family: Precup
- given: Yee Whye
  family: Teh
author:
- given: Andreas
  family: Loukas
date: 2017-07-17
container-title: Proceedings of the 34th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 7
  - 17
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---

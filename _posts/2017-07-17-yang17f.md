---
title: A Richer Theory of Convex Constrained Optimization with Reduced Projections
  and Improved Rates
booktitle: Proceedings of the 34th International Conference on Machine Learning
year: '2017'
volume: '70'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v70/yang17f/yang17f.pdf
url: http://proceedings.mlr.press/v70/yang17f.html
abstract: This paper focuses on convex constrained optimization problems, where the
  solution is subject to a convex inequality constraint. In particular, we aim at
  challenging problems for which both projection into the constrained domain and a
  linear optimization under the inequality constraint are time-consuming, which render
  both projected gradient methods and conditional gradient methods (a.k.a. the Frank-Wolfe
  algorithm) expensive. In this paper, we develop projection reduced optimization
  algorithms for both smooth and non-smooth optimization with improved convergence
  rates under a certain regularity condition of the constraint function. We first
  present a general theory of optimization with only one projection. Its application
  to smooth optimization with only one projection yields $O(1/ε)$ iteration complexity,
  which improves over the $O(1/ε^2)$ iteration complexity established before for non-smooth
  optimization and can be further reduced under strong convexity. Then we introduce
  a local error bound condition and develop faster algorithms for non-strongly convex
  optimization at the price of a logarithmic number of projections. In particular,
  we achieve an iteration complexity of $\widetilde O(1/ε^2(1-θ))$ for non-smooth
  optimization and $\widetilde O(1/ε^1-θ)$ for smooth optimization, where $θ∈(0,1]$
  appearing the local error bound condition characterizes the functional local growth
  rate around the optimal solutions. Novel applications in solving the constrained
  $\ell_1$ minimization problem and a positive semi-definite constrained distance
  metric learning problem demonstrate that the proposed algorithms achieve significant
  speed-up compared with previous algorithms.
layout: inproceedings
id: yang17f
tex_title: A Richer Theory of Convex Constrained Optimization with Reduced Projections
  and Improved Rates
firstpage: 3901
lastpage: 3910
page: 3901-3910
order: 3901
cycles: false
editor:
- given: Doina
  family: Precup
- given: Yee Whye
  family: Teh
author:
- given: Tianbao
  family: Yang
- given: Qihang
  family: Lin
- given: Lijun
  family: Zhang
date: 2017-07-17
container-title: Proceedings of the 34th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 7
  - 17
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---

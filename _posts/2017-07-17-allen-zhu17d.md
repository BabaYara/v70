---
title: 'Follow the Compressed Leader: Faster Online Learning of Eigenvectors and Faster
  MMWU'
booktitle: Proceedings of the 34th International Conference on Machine Learning
year: '2017'
volume: '70'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v70/allen-zhu17d/allen-zhu17d.pdf
url: http://proceedings.mlr.press/v70/allen-zhu17d.html
abstract: The online problem of computing the top eigenvector is fundamental to machine
  learning. The famous matrix-multiplicative-weight-update (MMWU) framework solves
  this online problem and gives optimal regret. However, since MMWU runs very slow
  due to the computation of matrix exponentials, researchers proposed the follow-the-perturbed-leader
  (FTPL) framework which is faster, but a factor $\sqrt{d}$ worse than the optimal
  regret for dimension-$d$ matrices. We propose a <em>follow-the-compressed-leader</em>
  framework which, not only matches the optimal regret of MMWU (up to polylog factors),
  but runs no slower than FTPL. Our main idea is to “compress” the MMWU strategy to
  dimension 3 in the adversarial setting, or dimension 1 in the stochastic setting.
  This resolves an open question regarding how to obtain both (nearly) optimal and
  efficient algorithms for the online eigenvector problem.
layout: inproceedings
id: allen-zhu17d
tex_title: 'Follow the Compressed Leader: Faster Online Learning of Eigenvectors and
  Faster {MMWU}'
firstpage: 116
lastpage: 125
page: 116-125
order: 116
cycles: false
editor:
- given: Doina
  family: Precup
- given: Yee Whye
  family: Teh
author:
- given: Zeyuan
  family: Allen-Zhu
- given: Yuanzhi
  family: Li
date: 2017-07-17
container-title: Proceedings of the 34th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 7
  - 17
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---

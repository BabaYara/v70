---
title: 'Averaged-DQN: Variance Reduction and Stabilization for Deep Reinforcement
  Learning'
booktitle: Proceedings of the 34th International Conference on Machine Learning
year: '2017'
volume: '70'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v70/anschel17a/anschel17a.pdf
url: http://proceedings.mlr.press/v70/anschel17.html
abstract: Instability and variability of Deep Reinforcement Learning (DRL) algorithms
  tend to adversely affect their performance. Averaged-DQN is a simple extension to
  the DQN algorithm, based on averaging previously learned Q-values estimates, which
  leads to a more stable training procedure and improved performance by reducing approximation
  error variance in the target values. To understand the effect of the algorithm,
  we examine the source of value function estimation errors and provide an analytical
  comparison within a simplified model. We further present experiments on the Arcade
  Learning Environment benchmark that demonstrate significantly improved stability
  and performance due to the proposed extension.
layout: inproceedings
id: anschel17a
tex_title: 'Averaged-{DQN}: Variance Reduction and Stabilization for Deep Reinforcement
  Learning'
firstpage: 176
lastpage: 185
page: 176-185
order: 176
cycles: false
editor:
- given: Doina
  family: Precup
- given: Yee Whye
  family: Teh
author:
- given: Oron
  family: Anschel
- given: Nir
  family: Baram
- given: Nahum
  family: Shimkin
date: 2017-07-17
container-title: Proceedings of the 34th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 7
  - 17
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v70/anschel17a/anschel17a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---

---
title: Uniform Deviation Bounds for k-Means Clustering
booktitle: Proceedings of the 34th International Conference on Machine Learning
year: '2017'
volume: '70'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v70/bachem17a/bachem17a.pdf
url: http://proceedings.mlr.press/v70/bachem17a.html
abstract: Uniform deviation bounds limit the difference between a model’s expected
  loss and its loss on an empirical sample \emphuniformly for all models in a learning
  problem. In this paper, we provide a novel framework to obtain uniform deviation
  bounds for loss functions which are \emphunbounded. As a result, we obtain competitive
  uniform deviation bounds for k-Means clustering under weak assumptions on the underlying
  distribution. If the fourth moment is bounded, we prove a rate of $O(m^-1/2)$ compared
  to the previously known $O(m^-1/4)$ rate. Furthermore, we show that the rate also
  depends on the kurtosis – the normalized fourth moment which measures the “tailedness”
  of a distribution. We also provide improved rates under progressively stronger assumptions,
  namely, bounded higher moments, subgaussianity and bounded support of the underlying
  distribution.
layout: inproceedings
id: bachem17a
tex_title: Uniform Deviation Bounds for k-Means Clustering
firstpage: 283
lastpage: 291
page: 283-291
order: 283
cycles: false
editor:
- given: Doina
  family: Precup
- given: Yee Whye
  family: Teh
author:
- given: Olivier
  family: Bachem
- given: Mario
  family: Lucic
- given: S. Hamed
  family: Hassani
- given: Andreas
  family: Krause
date: 2017-07-17
container-title: Proceedings of the 34th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 7
  - 17
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v70/bachem17a/bachem17a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---

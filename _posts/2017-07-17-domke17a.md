---
title: A Divergence Bound for Hybrids of MCMC and Variational Inference and an Application
  to Langevin Dynamics and SGVI
booktitle: Proceedings of the 34th International Conference on Machine Learning
year: '2017'
volume: '70'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v70/domke17a/domke17a.pdf
url: http://proceedings.mlr.press/v70/domke17.html
abstract: Two popular classes of methods for approximate inference are Markov chain
  Monte Carlo (MCMC) and variational inference. MCMC tends to be accurate if run for
  a long enough time, while variational inference tends to give better approximations
  at shorter time horizons. However, the amount of time needed for MCMC to exceed
  the performance of variational methods can be quite high, motivating more fine-grained
  tradeoffs. This paper derives a distribution over variational parameters, designed
  to minimize a bound on the divergence between the resulting marginal distribution
  and the target, and gives an example of how to sample from this distribution in
  a way that interpolates between the behavior of existing methods based on Langevin
  dynamics and stochastic gradient variational inference (SGVI).
layout: inproceedings
id: domke17a
tex_title: A Divergence Bound for Hybrids of {MCMC} and Variational Inference and
  an Application to {L}angevin Dynamics and {SGVI}
firstpage: 1029
lastpage: 1038
page: 1029-1038
order: 1029
cycles: false
editor:
- given: Doina
  family: Precup
- given: Yee Whye
  family: Teh
author:
- given: Justin
  family: Domke
date: 2017-07-17
container-title: Proceedings of the 34th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 7
  - 17
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v70/domke17a/domke17a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---

---
title: Unifying Task Specification in Reinforcement Learning
booktitle: Proceedings of the 34th International Conference on Machine Learning
year: '2017'
volume: '70'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v70/white17a/white17a.pdf
url: http://proceedings.mlr.press/v70/white17a.html
abstract: Reinforcement learning tasks are typically specified as Markov decision
  processes. This formalism has been highly successful, though specifications often
  couple the dynamics of the environment and the learning objective. This lack of
  modularity can complicate generalization of the task specification, as well as obfuscate
  connections between different task settings, such as episodic and continuing. In
  this work, we introduce the RL task formalism, that provides a unification through
  simple constructs including a generalization to transition-based discounting. Through
  a series of examples, we demonstrate the generality and utility of this formalism.
  Finally, we extend standard learning constructs, including Bellman operators, and
  extend some seminal theoretical results, including approximation errors bounds.
  Overall, we provide a well-understood and sound formalism on which to build theoretical
  results and simplify algorithm use and development.
layout: inproceedings
id: white17a
tex_title: Unifying Task Specification in Reinforcement Learning
firstpage: 3742
lastpage: 3750
page: 3742-3750
order: 3742
cycles: false
editor:
- given: Doina
  family: Precup
- given: Yee Whye
  family: Teh
author:
- given: Martha
  family: White
date: 2017-07-17
container-title: Proceedings of the 34th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 7
  - 17
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v70/white17a/white17a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---

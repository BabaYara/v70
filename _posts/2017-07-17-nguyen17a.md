---
title: The Loss Surface of Deep and Wide Neural Networks
booktitle: Proceedings of the 34th International Conference on Machine Learning
year: '2017'
volume: '70'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v70/nguyen17a/nguyen17a.pdf
url: http://proceedings.mlr.press/v70/nguyen17a.html
abstract: While the optimization problem behind deep neural networks is highly non-convex,
  it is frequently observed in practice that training deep networks seems possible
  without getting stuck in suboptimal points. It has been argued that this is the
  case as all local minima are close to being globally optimal. We show that this
  is (almost) true, in fact almost all local minima are globally optimal, for a fully
  connected network with squared loss and analytic activation function given that
  the number of hidden units of one layer of the network is larger than the number
  of training points and the network structure from this layer on is pyramidal.
layout: inproceedings
id: nguyen17a
tex_title: The Loss Surface of Deep and Wide Neural Networks
firstpage: 2603
lastpage: 2612
page: 2603-2612
order: 2603
cycles: false
editor:
- given: Doina
  family: Precup
- given: Yee Whye
  family: Teh
author:
- given: Quynh
  family: Nguyen
- given: Matthias
  family: Hein
date: 2017-07-17
container-title: Proceedings of the 34th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 7
  - 17
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v70/nguyen17a/nguyen17a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---

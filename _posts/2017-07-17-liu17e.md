---
title: 'Dual Iterative Hard Thresholding: From Non-convex Sparse Minimization to Non-smooth
  Concave Maximization'
booktitle: Proceedings of the 34th International Conference on Machine Learning
year: '2017'
volume: '70'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v70/liu17e/liu17e.pdf
url: http://proceedings.mlr.press/v70/liu17e.html
abstract: Iterative Hard Thresholding (IHT) is a class of projected gradient descent
  methods for optimizing sparsity-constrained minimization models, with the best known
  efficiency and scalability in practice. As far as we know, the existing IHT-style
  methods are designed for sparse minimization in primal form. It remains open to
  explore duality theory and algorithms in such a non-convex and NP-hard setting.
  In this article, we bridge the gap by establishing a duality theory for sparsity-constrained
  minimization with $\ell_2$-regularized objective and proposing an IHT-style algorithm
  for dual maximization. Our sparse duality theory provides a set of sufficient and
  necessary conditions under which the original NP-hard/non-convex problem can be
  equivalently solved in a dual space. The proposed dual IHT algorithm is a super-gradient
  method for maximizing the non-smooth dual objective. An interesting finding is that
  the sparse recovery performance of dual IHT is invariant to the Restricted Isometry
  Property (RIP), which is required by all the existing primal IHT without sparsity
  relaxation. Moreover, a stochastic variant of dual IHT is proposed for large-scale
  stochastic optimization. Numerical results demonstrate that dual IHT algorithms
  can achieve more accurate model estimation given small number of training data and
  have higher computational efficiency than the state-of-the-art primal IHT-style
  algorithms.
layout: inproceedings
id: liu17e
tex_title: 'Dual Iterative Hard Thresholding: From Non-convex Sparse Minimization
  to Non-smooth Concave Maximization'
firstpage: 2179
lastpage: 2187
page: 2179-2187
order: 2179
cycles: false
editor:
- given: Doina
  family: Precup
- given: Yee Whye
  family: Teh
author:
- given: Bo
  family: Liu
- given: Xiao-Tong
  family: Yuan
- given: Lezi
  family: Wang
- given: Qingshan
  family: Liu
- given: Dimitris N.
  family: Metaxas
date: 2017-07-17
container-title: Proceedings of the 34th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 7
  - 17
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v70/liu17e/liu17e-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---

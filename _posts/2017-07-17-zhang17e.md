---
title: 'ZipML: Training Linear Models with End-to-End Low Precision, and a Little
  Bit of Deep Learning'
booktitle: Proceedings of the 34th International Conference on Machine Learning
year: '2017'
volume: '70'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v70/zhang17e/zhang17e.pdf
url: http://proceedings.mlr.press/v70/zhang17e.html
abstract: 'Recently there has been significant interest in training machine-learning
  models at low precision: by reducing precision, one can reduce computation and communication
  by one order of magnitude. We examine training at reduced precision, both from a
  theoretical and practical perspective, and ask: is it possible to train models at
  end-to-end low precision with provable guarantees? Can this lead to consistent order-of-magnitude
  speedups? We mainly focus on linear models, and the answer is yes for linear models.
  We develop a simple framework called ZipML based on one simple but novel strategy
  called double sampling. Our ZipML framework is able to execute training at low precision
  with no bias, guaranteeing convergence, whereas naive quantization would introduce
  significant bias. We validate our framework across a range of applications, and
  show that it enables an FPGA prototype that is up to $6.5\times$ faster than an
  implementation using full 32-bit precision. We further develop a variance-optimal
  stochastic quantization strategy and show that it can make a significant difference
  in a variety of settings. When applied to linear models together with double sampling,
  we save up to another $1.7\times$ in data movement compared with uniform quantization.
  When training deep networks with quantized models, we achieve higher accuracy than
  the state-of-the-art XNOR-Net.'
layout: inproceedings
id: zhang17e
tex_title: "{Z}ip{ML}: Training Linear Models with End-to-End Low Precision, and a
  Little Bit of Deep Learning"
firstpage: 4035
lastpage: 4043
page: 4035-4043
order: 4035
cycles: false
editor:
- given: Doina
  family: Precup
- given: Yee Whye
  family: Teh
author:
- given: Hantian
  family: Zhang
- given: Jerry
  family: Li
- given: Kaan
  family: Kara
- given: Dan
  family: Alistarh
- given: Ji
  family: Liu
- given: Ce
  family: Zhang
date: 2017-07-17
container-title: Proceedings of the 34th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 7
  - 17
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---

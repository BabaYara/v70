---
title: 'Re-revisiting Learning on Hypergraphs: Confidence Interval and Subgradient
  Method'
booktitle: Proceedings of the 34th International Conference on Machine Learning
year: '2017'
volume: '70'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v70/zhang17d/zhang17d.pdf
url: http://proceedings.mlr.press/v70/zhang17d.html
abstract: We revisit semi-supervised learning on hypergraphs. Same as previous approaches,
  our method uses a convex program whose objective function is not everywhere differentiable.
  We exploit the non-uniqueness of the optimal solutions, and consider confidence
  intervals which give the exact ranges that unlabeled vertices take in any optimal
  solution. Moreover, we give a much simpler approach for solving the convex program
  based on the subgradient method. Our experiments on real-world datasets confirm
  that our confidence interval approach on hypergraphs outperforms existing methods,
  and our sub-gradient method gives faster running times when the number of vertices
  is much larger than the number of edges.
layout: inproceedings
id: zhang17d
tex_title: 'Re-revisiting Learning on Hypergraphs: Confidence Interval and Subgradient
  Method'
firstpage: 4026
lastpage: 4034
page: 4026-4034
order: 4026
cycles: false
editor:
- given: Doina
  family: Precup
- given: Yee Whye
  family: Teh
author:
- given: Chenzi
  family: Zhang
- given: Shuguang
  family: Hu
- given: Zhihao Gavin
  family: Tang
- given: T-H. Hubert
  family: Chan
date: 2017-07-17
container-title: Proceedings of the 34th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 7
  - 17
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---

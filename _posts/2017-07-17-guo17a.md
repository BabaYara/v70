---
title: On Calibration of Modern Neural Networks
booktitle: Proceedings of the 34th International Conference on Machine Learning
year: '2017'
volume: '70'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v70/guo17a/guo17a.pdf
url: http://proceedings.mlr.press/v70/guo17a.html
abstract: 'Confidence calibration – the problem of predicting probability estimates
  representative of the true correctness likelihood – is important for classification
  models in many applications. We discover that modern neural networks, unlike those
  from a decade ago, are poorly calibrated. Through extensive experiments, we observe
  that depth, width, weight decay, and Batch Normalization are important factors influencing
  calibration. We evaluate the performance of various post-processing calibration
  methods on state-of-the-art architectures with image and document classification
  datasets. Our analysis and experiments not only offer insights into neural network
  learning, but also provide a simple and straightforward recipe for practical settings:
  on most datasets, temperature scaling – a single-parameter variant of Platt Scaling
  – is surprisingly effective at calibrating predictions.'
layout: inproceedings
id: guo17a
tex_title: On Calibration of Modern Neural Networks
firstpage: 1321
lastpage: 1330
page: 1321-1330
order: 1321
cycles: false
editor:
- given: Doina
  family: Precup
- given: Yee Whye
  family: Teh
author:
- given: Chuan
  family: Guo
- given: Geoff
  family: Pleiss
- given: Yu
  family: Sun
- given: Kilian Q.
  family: Weinberger
date: 2017-07-17
container-title: Proceedings of the 34th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 7
  - 17
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v70/guo17a/guo17a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---

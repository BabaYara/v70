---
title: A Laplacian Framework for Option Discovery in Reinforcement Learning
booktitle: Proceedings of the 34th International Conference on Machine Learning
year: '2017'
volume: '70'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v70/machado17a/machado17a.pdf
url: http://proceedings.mlr.press/v70/machado17a.html
abstract: Representation learning and option discovery are two of the biggest challenges
  in reinforcement learning (RL). Proto-value functions (PVFs) are a well-known approach
  for representation learning in MDPs. In this paper we address the option discovery
  problem by showing how PVFs implicitly define options. We do it by introducing eigenpurposes,
  intrinsic reward functions derived from the learned representations. The options
  discovered from eigenpurposes traverse the principal directions of the state space.
  They are useful for multiple tasks because they are discovered without taking the
  environmentâ€™s rewards into consideration. Moreover, different options act at different
  time scales, making them helpful for exploration. We demonstrate features of eigenpurposes
  in traditional tabular domains as well as in Atari 2600 games.
layout: inproceedings
id: machado17a
tex_title: A {L}aplacian Framework for Option Discovery in Reinforcement Learning
firstpage: 2295
lastpage: 2304
page: 2295-2304
order: 2295
cycles: false
editor:
- given: Doina
  family: Precup
- given: Yee Whye
  family: Teh
author:
- given: Marlos C.
  family: Machado
- given: Marc G.
  family: Bellemare
- given: Michael
  family: Bowling
date: 2017-07-17
container-title: Proceedings of the 34th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 7
  - 17
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v70/machado17a/machado17a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---

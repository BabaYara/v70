---
title: 'Learning Gradient Descent: Better Generalization and Longer Horizons'
booktitle: Proceedings of the 34th International Conference on Machine Learning
year: '2017'
volume: '70'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v70/lv17a/lv17a.pdf
url: http://proceedings.mlr.press/v70/lv17a.html
abstract: Training deep neural networks is a highly nontrivial task, involving carefully
  selecting appropriate training algorithms, scheduling step sizes and tuning other
  hyperparameters. Trying different combinations can be quite labor-intensive and
  time consuming. Recently, researchers have tried to use deep learning algorithms
  to exploit the landscape of the loss function of the training problem of interest,
  and learn how to optimize over it in an automatic way. In this paper, we propose
  a new learning-to-learn model and some useful and practical tricks. Our optimizer
  outperforms generic, hand-crafted optimization algorithms and state-of-the-art learning-to-learn
  optimizers by DeepMind in many tasks. We demonstrate the effectiveness of our algorithms
  on a number of tasks, including deep MLPs, CNNs, and simple LSTMs.
layout: inproceedings
id: lv17a
tex_title: 'Learning Gradient Descent: Better Generalization and Longer Horizons'
firstpage: 2247
lastpage: 2255
page: 2247-2255
order: 2247
cycles: false
editor:
- given: Doina
  family: Precup
- given: Yee Whye
  family: Teh
author:
- given: Kaifeng
  family: Lv
- given: Shunhua
  family: Jiang
- given: Jian
  family: Li
date: 2017-07-17
container-title: Proceedings of the 34th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 7
  - 17
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---

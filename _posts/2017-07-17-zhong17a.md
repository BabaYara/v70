---
title: Recovery Guarantees for One-hidden-layer Neural Networks
booktitle: Proceedings of the 34th International Conference on Machine Learning
year: '2017'
volume: '70'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v70/zhong17a/zhong17a.pdf
url: http://proceedings.mlr.press/v70/zhong17.html
abstract: In this paper, we consider regression problems with one-hidden-layer neural
  networks (1NNs). We distill some properties of activation functions that lead to   \emphlocal
  strong convexity in the neighborhood of the ground-truth parameters for the 1NN
  squared-loss objective and most popular nonlinear activation functions  satisfy
  the distilled properties, including rectified linear units (ReLUs), leaky ReLUs,
  squared ReLUs and sigmoids. For activation functions that are also smooth, we show
  \emphlocal linear convergence guarantees of gradient descent under a resampling
  rule. For homogeneous activations, we show tensor methods are able to initialize
  the parameters to fall into the local strong convexity region. As a result, tensor
  initialization followed by gradient descent is guaranteed to recover the ground
  truth with sample complexity $ d ⋅\log(1/ε) ⋅\mathrmpoly(k,λ)$ and computational
  complexity $n⋅d ⋅\mathrmpoly(k,λ) $ for smooth  homogeneous activations with high
  probability, where $d$ is the dimension of the input, $k$ ($k≤d$) is the number
  of hidden nodes, $λ$ is a conditioning  property of the ground-truth parameter matrix
  between the input layer and the hidden layer, $ε$ is the targeted precision and
  $n$ is the number of samples. To the best of our knowledge, this is the first work
  that provides recovery guarantees for 1NNs with both sample complexity and computational
  complexity \emphlinear in the input dimension and \emphlogarithmic in the precision.
layout: inproceedings
id: zhong17a
tex_title: Recovery Guarantees for One-hidden-layer Neural Networks
firstpage: 4140
lastpage: 4149
page: 4140-4149
order: 4140
cycles: false
editor:
- given: Doina
  family: Precup
- given: Yee Whye
  family: Teh
author:
- given: Kai
  family: Zhong
- given: Zhao
  family: Song
- given: Prateek
  family: Jain
- given: Peter L.
  family: Bartlett
- given: Inderjit S.
  family: Dhillon
date: 2017-07-17
container-title: Proceedings of the 34th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 7
  - 17
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
